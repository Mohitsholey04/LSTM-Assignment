{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries  ------------------------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.metrics import Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset of student academic performance metrics  ----------------------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the dataset is stored in a CSV file named 'student_performance.csv'\n",
    "df = pd.read_csv('student_performance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data (if needed) by handling missing values, outliers, etc. ----------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Handling missing values\n",
    "# Option 1: Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Option 2: Fill missing values with mean, median or mode\n",
    "# Example: Fill missing values in 'column_name' with mean\n",
    "df['column_name'].fillna(df['column_name'].mean(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data (if needed) by handling missing values, outliers, etc.  -------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers\n",
    "# Option 1: Using IQR (Interquartile Range)\n",
    "Q1 = df['column_name'].quantile(0.25)\n",
    "Q3 = df['column_name'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['column_name'] < lower_bound) | (df['column_name'] > upper_bound)]\n",
    "print(\"Outliers:\\n\", outliers)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data cleaning operations as per the specific requirements of the dataset.  ------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Z-score\n",
    "from scipy.stats import zscore\n",
    "z_scores = zscore(df['column_name'])\n",
    "outliers = df[np.abs(z_scores) > 3]\n",
    "print(\"Outliers:\\n\", outliers)\n",
    "\n",
    "# Handling outliers\n",
    "# Option 1: Remove outliers\n",
    "df = df[(df['column_name'] > lower_bound) & (df['column_name'] < upper_bound)]\n",
    "\n",
    "# Option 2: Cap or floor outliers\n",
    "df['column_name'] = np.where(df['column_name'] < lower_bound, lower_bound, df['column_name'])\n",
    "df['column_name'] = np.where(df['column_name'] > upper_bound, upper_bound, df['column_name'])\n",
    "\n",
    "# Other data cleaning operations\n",
    "# Example: Remove unnecessary columns\n",
    "df = df.drop(['unnecessary_column1', 'unnecessary_column2'], axis=1)\n",
    "\n",
    "# Example: Convert data types\n",
    "df['column_name'] = df['column_name'].astype(int)\n",
    "\n",
    "# Example: Renaming columns\n",
    "df = df.rename(columns={'old_column_name': 'new_column_name'})\n",
    "\n",
    "# Example: Fixing inconsistent values\n",
    "df['column_name'] = df['column_name'].replace('incorrect_value', 'correct_value')\n",
    "\n",
    "# Example: Removing duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Example: Reordering columns\n",
    "df = df[['column_name1', 'column_name2', 'column_name3']]\n",
    "\n",
    "# Saving cleaned dataset\n",
    "df.to_csv('cleaned_dataset.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate academic performance of students ---------------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign values to variables\n",
    "a = 1-1 semester percentage\n",
    "b = 1-2 semester percentage\n",
    "c = 2-1 semester percentage\n",
    "d = 2-2 semester percentage\n",
    "e = 3-1 semester percentage\n",
    "f = 3-2 semester Percentage\n",
    "g = Attendance percentage\n",
    "h = extracurricular activities\n",
    "i = Academic awards and achievements\n",
    "j = Coding skills\n",
    "k = [a, b, c, d, e, f] # semester grades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dropout\n",
    "dropout = 1 if min(k) < 35 and g < 30 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate good performance\n",
    "good_performance = 1 if all(grade > 60 for grade in k) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate poor performance\n",
    "poor_performance = 1 if max(k) < 40 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate support required\n",
    "support_required = 1 if any(40 <= grade < 60 for grade in k) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eligibility for placement\n",
    "eligible_for_placement = 1 if all(grade > 65 for grade in k) and (j or i or h) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(\"Dropout: \", dropout)\n",
    "print(\"Good Performance: \", good_performance)\n",
    "print(\"Poor Performance: \", poor_performance)\n",
    "print(\"Support Required: \", support_required)\n",
    "print(\"Eligible for Placement: \", eligible_for_placement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the output of academic performance categories  ---------------------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of dropouts: \", df['dropout'].sum())\n",
    "print(\"Number of good performers: \", df['good_performance'].sum())\n",
    "print(\"Number of poor performers: \", df['poor_performance'].sum())\n",
    "print(\"Number of students requiring support: \", df['support_required'].sum())\n",
    "print(\"Number of students eligible for placement: \", df['eligible_for_placement'].sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize critical values as graphs across all students  ---------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize critical values using boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='student_id', y='critical_value', data=df)\n",
    "plt.title('Critical Values Distribution by Student')\n",
    "plt.xlabel('Student ID')\n",
    "plt.ylabel('Critical Value')\n",
    "plt.show()\n",
    "\n",
    "# Visualize critical values using violinplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(x='student_id', y='critical_value', data=df)\n",
    "plt.title('Critical Values Distribution by Student')\n",
    "plt.xlabel('Student ID')\n",
    "plt.ylabel('Critical Value')\n",
    "plt.show()\n",
    "\n",
    "# Visualize critical values using swarmplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.swarmplot(x='student_id', y='critical_value', data=df)\n",
    "plt.title('Critical Values Distribution by Student')\n",
    "plt.xlabel('Student ID')\n",
    "plt.ylabel('Critical Value')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create appropriate plots to visualize the distribution of performance metrics ------------------------>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df['performance_metric'], bins=10, kde=True)\n",
    "plt.title('Distribution of Performance Metrics')\n",
    "plt.xlabel('Performance Metric')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Example: Visualize performance metrics using a boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(df['performance_metric'])\n",
    "plt.title('Distribution of Performance Metrics')\n",
    "plt.xlabel('Performance Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# Example: Visualize performance metrics using a violinplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(df['performance_metric'])\n",
    "plt.title('Distribution of Performance Metrics')\n",
    "plt.xlabel('Performance Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for LSTM model  -------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values  # Input features - semester percentages\n",
    "y = df[['good_performance', 'poor_performance', 'support_required', 'dropout']].values  # Target variables - performance categories\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets  ----------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape input data into the required format for LSTM modeling  ---------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = X.shape[1]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape(X_train.shape[0], n_timesteps, n_features)\n",
    "X_test = X_test.reshape(X_test.shape[0], n_timesteps, n_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and compile the LSTM model  ---------------------------------------------------->>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X--------------------------------------------------------- END OF CODE ---------------------------------------------------X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
